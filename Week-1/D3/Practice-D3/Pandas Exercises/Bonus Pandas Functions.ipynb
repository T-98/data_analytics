{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <font color='#eb3483'>Bonus Pandas Functions  </font>\n",
    "\n",
    "Pandas has way more functions than we can even hope to cover in class. Here are a few that might be helpful during your homework and class project.    \n",
    "    \n",
    "1. Isin\n",
    "1. Working with different data types\n",
    "1. Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#eb3483'> 1. Isin </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if an element belongs to a python list like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"potato\" in [\"potato\", \"tomato\", \"lettuce\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a similar approach with pandas dataframes using `.isin`. For example, if we want to select those listings where the neighborhood is in a specific list we can do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/airbnb.csv\", index_col=\"room_id\").sort_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "favorite_neighbourhoods = [\"Belém\", \"Parque das Nações\"]\n",
    "\n",
    "listings_i_like = df[df.neighborhood.isin(favorite_neighbourhoods)]\n",
    "\n",
    "listings_i_like.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### <font color='#eb3483'> Filtering based on datatypes </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the method `select_dtypes` to select those columns that have specific types. \n",
    "\n",
    "For example, if we want to select only the columns that are floats, we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=[float]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the parameter `exclude` to filter excluding certain data types. \n",
    "\n",
    "For example, if we want to exclude those columns that are python objects (and strings are objects), we can do so like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.select_dtypes(exclude=[object]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#eb3483'> 2. Working with Different Data Types </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we have the following dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_data = pd.DataFrame({\n",
    "    \"worked_hours\": [5, 5.2, \"sick\", 8, 10],\n",
    "    \"work_date\": [\"2020-01-02\", \"208-01-02\", \"2020-01-04\", \"2020-01-05\", \"2020-01-06\"],\n",
    "    \"employee\": [\"taryn morris/ix\", \"connor lawless/ix\", \"connor lawless/ix\", \"martha/ix\", \"martha/ix\"],\n",
    "    \"customer_review\": [5,4,0,2,4]\n",
    "})\n",
    "\n",
    "unprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the dataset, it is obvious that the variable `work_date` is a date, `worked_hours` is a number and `employee` is a string. However, because of errors on the data, Pandas has considered all columns as strings (object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn columns into numbers with `pd.to_numeric`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_numeric(unprocessed_data.worked_hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `to_numeric` fails if any value can't be converted. We can change the conversion strategy by specifying the error strategy (with the argument `errors`). For example, if we use `errors=\"coerce\"` pandas will convert those values it can and will convert the rest to null values (NaN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.to_numeric(unprocessed_data.worked_hours, errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#eb3483'> Strings </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those columns that are strings have a set of special methods, available under the `.str` name.\n",
    "\n",
    "For example, if we want to set the employee name to be capitalize, we can use `.str.lower()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_data.employee.str.capitalize().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we want to replace the spaces on the name with a `_`, we can do so with `.replace()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_data.employee = unprocessed_data.employee.str.replace(\" \", \"_\").str.capitalize()\n",
    "unprocessed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many string methods available, for example, we can use `.str.strip()` to remove the `/apt` at the end of the room_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_data.employee.str.strip(\"/ix*\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `.str.cat` to concatenate strings. For example, if we want to set the index to `employee_work_date` we can do so as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_data = unprocessed_data.set_index('employee', drop = False)\n",
    "unprocessed_data.index = unprocessed_data.index.str.cat(unprocessed_data.work_date.astype(str), sep=\"_\")\n",
    "unprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#eb3483'>  Datetimes </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same way, we can convert dates to datetime objects (a datatype specific for dates). We can use `pd.to_datetime` to do so. It will coerce invalid dates into nulls (NaT, `not a time`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(unprocessed_data.work_date, errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way we can transform the dates into actual date objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_data[\"work_date\"] = pd.to_datetime(unprocessed_data.work_date, errors=\"coerce\")\n",
    "unprocessed_data[\"worked_hours\"] = pd.to_numeric(unprocessed_data.worked_hours, errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datetime columns have additional functionality, similarly to the `str.` methods with strings. We can access the datetime methods via the attribute `.dt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can find the day of week with `.dt.dayofweek`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_data.work_date.dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#eb3483'>  Categories </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/airbnb.csv\", index_col=\"room_id\").sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas can encode categorical variables in two ways, regular objects (strings), or **category**. \n",
    "Pandas provide us with a category dtype for categorical data:\n",
    "\n",
    "Why would we want to use categories?\n",
    "\n",
    "    Easily identify and signal categorical columns for processing and other Python libraries\n",
    "    Converting a string variable with a few different values to a categorical variable saves memory\n",
    "    By converting to a categorical we can specify an order on the categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.memory_usage(index=True, deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah, neighborhood is taking up a lot of memory. We always want to be as efficient as possible.\n",
    "\n",
    "Let's try encode the room type and neighbourhood as categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({\"room_type\": \"category\", \"neighborhood\": \"category\"})\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see we get additional information when describing categories, and not only that, the 2 columns now take significantly less memory space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.memory_usage(index=True, deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas also works well for ordinal variables (variables that have a ranking). We can assign an order to a categorical variable. For example, we can set the variable `room_type` as an ordinal variable `(Shared Room < Private Room < Entire Home)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.room_type.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default a categorical column is not ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.room_type.cat.ordered # let slook up this function in help - press click on the function and shift+tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assign the order by doing `.cat.set_categories` and passing the list of levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.room_type = df.room_type.cat.set_categories([\"Shared room\",\"Private room\", \"Entire home/apt\"], ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.room_type.cat.ordered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now because we know the sorting order of the categories, we can filter the ordinal column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.room_type>\"Shared room\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reading a csv, we can also specify dtypes so we can directly create category columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/airbnb.csv\", dtype={\n",
    "                                        \"room_type\": \"category\",\n",
    "                                        \"neighborhood\": \"category\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#eb3483'> Rename </font>\n",
    "\n",
    "`.rename` allows us to rename either the row index or the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rename columns very easily this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"neighborhood\": \"neighbourhood\", \"reviews\": \"number_reviews\"})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also rename values in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexed_neighbourhood = df.set_index(\"neighbourhood\")\n",
    "df_indexed_neighbourhood.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_indexed_neighbourhood.rename({\"Belém\": \"Belem\", \"Santa Maria Maior\": \"Saint Mary\"}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#eb3483'> 3. Merge </font>\n",
    "\n",
    "`merge` allows us to join two dataframes based on one or two columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to load an additional dataset that contains the population in every Lisbon district (taken from [here](https://www.citypopulation.de/php/portugal-lisboa.php))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/airbnb.csv\", index_col=\"room_id\").sort_index()\n",
    "populations = pd.read_csv(\"data/population_lisboa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do a merge between the Airbnb listings and the populations.\n",
    "\n",
    "merge works as follows:\n",
    "\n",
    "```\n",
    "pd.merge(left_dataframe,right_dataframe,join_type)\n",
    "```\n",
    "\n",
    "merge has different parameters:\n",
    "\n",
    "`left_on` is the name of the column for the left dataframe.\n",
    "`right_on` is the name of the column for the right dataframe.\n",
    "`how` specifies the join type, how to make the union:\n",
    " - `full` takes all rows for both dataframes whether the values on the join columns exist on both dataframes or not.\n",
    " - `inner` takes only the rows on the dataframews where the values on the join columns exist on both dataframes. This is the default value.\n",
    " - `left` takes all rows for the left dataframe whether the values on the join columns exist on it dataframes or not.\n",
    " - `right` the opposite of `left`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_with_pop = pd.merge(df, populations, left_on=\"neighborhood\", right_on=\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_with_pop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_with_pop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how merge works.\n",
    "\n",
    "We can see the original number of neighbourhoods by doing `unique()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoud_list_all = df.neighborhood.unique()\n",
    "neighborhoud_list_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(neighborhoud_list_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there are 24 unique neighbourhoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many neighbourhoods we get when we do the merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoud_list_merge = listings_with_pop.neighborhood.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(neighborhoud_list_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there is a neighbourhood missing! Maybe there is one missing on the population dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(neighborhoud_list_all) - set(neighborhoud_list_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see *São Vicente* doesn't exist on the population dataset. When we do a merge, we do an `inner` merge by default, and it removes those rows whose shared column/s don't match. In this example `*São Vicente*` exists on the Airbnbn `neighborhood` column but doesn't exist on the populations dataframe `name` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_with_pop[listings_with_pop.neighborhood==\"São Vicente\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular example, the reference dataframe is the Airbnb, and we are just adding additional data to it. So we would rather keep those listings in São Vicente even though we wont have population information for them. In this case we will do a left merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_with_pop = pd.merge(df, populations, left_on=\"neighborhood\", right_on=\"name\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we kept all the listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "listings_with_pop[listings_with_pop.neighborhood==\"São Vicente\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
